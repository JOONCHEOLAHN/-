{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f69ec3-48c9-4e20-811d-82f52bb0f30d",
   "metadata": {},
   "source": [
    "## 크롤링에 필요한 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94884990-1af0-4b7e-b37d-3c645e0ee714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container{width:100% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import math\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "#-------------------- 주피터 셀 넓이 조절 ---------------------\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container{width:100% !important;}</style>\"))\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "# pip install selenium\n",
    "# pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b13a02-a42a-4622-b766-b6488ecb97c8",
   "metadata": {},
   "source": [
    "## 크롬 웹브라우저 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f5e6f4a-8669-4c07-9716-efeb94cd5d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"C:/AI/chromedriver_win32/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c97cbd8-dad5-48ba-81cc-0997a1348bab",
   "metadata": {},
   "source": [
    "## 네이버 로그인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3fc4b3e-1d4d-484f-a0ad-0722f00c22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 로그인 전용 화면\n",
    "# driver.get('https://nid.naver.com/nidlogin.login')\n",
    "# # 아이디와 비밀번호 입력\n",
    "# driver.find_element_by_name('id').send_keys('mingiraffe')\n",
    "# driver.find_element_by_name('pw').send_keys('rlfls9923!')\n",
    "# # 로그인 버튼 클릭\n",
    "# driver.find_element_by_css_selector('#log\\.login').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a32ba3-ba6a-4538-bc27-0a98f2bcc56a",
   "metadata": {},
   "source": [
    "## 사이트 주소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c98acf6-1f81-4925-a5fb-718ef681d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://cafe.naver.com/mbticafe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ae747f-07ca-45b0-8fc4-74881fb75f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab830b63-8be8-48da-961b-61b86d7a6586",
   "metadata": {},
   "source": [
    "## 게시판 클릭  --> 게시판 루프 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b39c56-aae4-4a00-97bb-166a685acd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'INFP ♧ ENFP'\n",
    "driver.find_element_by_link_text(keyword).click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70d588e-292f-4e8f-b8dc-d291b3838602",
   "metadata": {},
   "source": [
    "## 게시판 프레임 접근"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6284a5c2-5295-4b1c-8907-c8c11094ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.switch_to.frame(\"cafe_main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e91e82c-a883-4995-9b49-da0e6ee552b0",
   "metadata": {},
   "source": [
    "## 게시글 50개씩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60c120c6-9224-4d5a-9ecc-005b8c7dc724",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_css_selector(\"#listSizeSelectDiv\").click()\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/div/div[2]/div/div[3]/ul/li[7]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "181ac081-bea5-402e-978e-4420fd6602e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "클롤링 할 글 개수를 입력 : 5000\n"
     ]
    }
   ],
   "source": [
    "#crawling_list = []\n",
    "no_app = []\n",
    "links_app  = []\n",
    "crawling_no = int(input('클롤링 할 글 개수를 입력 :'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfb31f99-44a5-4a53-aa5f-63c141e20d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 page 크롤링 완료\n",
      "20 page 크롤링 완료\n",
      "30 page 크롤링 완료\n",
      "40 page 크롤링 완료\n",
      "50 page 크롤링 완료\n",
      "60 page 크롤링 완료\n",
      "70 page 크롤링 완료\n",
      "80 page 크롤링 완료\n",
      "90 page 크롤링 완료\n",
      "100 page 크롤링 완료\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16228/3221184620.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# 판다스화\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m df = pd.DataFrame({'번호':no_list,\n\u001b[0m\u001b[0;32m     38\u001b[0m                        '주소':links_list})\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# 필독, 공지 삭제\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ai\\pythonproject\\venv\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ai\\pythonproject\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         ]\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ai\\pythonproject\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ai\\pythonproject\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arrays must all be same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "# 크롤링 해야 할 페이지 계산\n",
    "crawling_page = int(math.ceil(crawling_no / 50)+1)   # 여기 수정\n",
    "\n",
    "try: \n",
    "    for page in range(1,crawling_page):\n",
    "        # 페이지 클릭\n",
    "        driver.find_element_by_link_text(str(page)).click()\n",
    "        time.sleep(1)\n",
    "        \n",
    "        no = [i.text for i in driver.find_elements_by_css_selector('.td_article')]\n",
    "        no_split = [ni.split()[0] for ni in no]\n",
    "\n",
    "        # href 링크 수집\n",
    "        article_list = driver.find_elements_by_css_selector('.td_article > div.board-list > div > a.article')\n",
    "        article_urls = [ i.get_attribute('href') for i in article_list ]    \n",
    "            \n",
    "        # 수집 데이터 append\n",
    "\n",
    "        links_app.append(article_urls)\n",
    "        \n",
    "        # 10페이지 마다 프린트 & 다음 페이지로 클릭\n",
    "        if str(page)[-1] == '0':\n",
    "            print(int(page), 'page 크롤링 완료')\n",
    "            driver.find_element_by_link_text('다음').click()\n",
    "# 더이상 페이지가 존재하지 않을 시\n",
    "except:\n",
    "    print('더이상 페이지가 존재하지 않음')\n",
    "\n",
    "# driver.close()\n",
    "    \n",
    "# 리스트안 리스트 분해\n",
    "no_list = sum(no_app, [])\n",
    "\n",
    "links_list = sum(links_app, [])\n",
    "\n",
    "# 판다스화\n",
    "df = pd.DataFrame({'번호':no_list,\n",
    "                         '주소':links_list})\n",
    "# 필독, 공지 삭제\n",
    "df = df.drop(df[df['번호'] == '필독'].index)\n",
    "df = df.drop(df[df['번호'] == '공지'].index)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print('글 ', len(df), '개 크롤링 완료. \\n크롤링 종료.', sep='')\n",
    "\n",
    "df\n",
    "\n",
    "# 저장\n",
    "df.to_excel('crawler_naver cafe_게시판 {}.xlsx'.format(keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2365c85d-e3cb-4d6f-ac92-c98fa270faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244fdee0-c3fd-4172-b929-d197bafe497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "df.to_excel('crawler_naver cafe_게시판 {}.xlsx'.format(keyword))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ef817-cc30-43fe-ae39-eade37bbd14a",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef5197-a9eb-4145-a123-5085000426c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요기까지\n",
    "asdfsdfasdfasdfsdfsdfsdaf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a6d8b-d146-4f55-96d7-a5198b8402c8",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de11f75-8fbe-4703-86ae-d04fc420e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['주소'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec39d76-6230-4cf4-a3a9-b82cf13efd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "add = df['주소']\n",
    "add = add.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9171288-f6cf-4eaa-b2b8-6bad8d179bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a803d8-7734-457a-b3a5-f948673ada89",
   "metadata": {},
   "source": [
    "## 클릭된 게시글로 접근"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68169d7f-3308-44af-9beb-746acb66b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a9ee3-9e4a-48a0-b166-df630647fbe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_list = []\n",
    "# Beautifulsoup 활용\n",
    "for link in add:\n",
    "    driver.get(link)\n",
    "\n",
    "    driver.switch_to.frame(\"cafe_main\")\n",
    "    html = driver.page_source\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd991f4-1d2d-4740-ac1b-60f286699793",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 게시글에서 제목 추출\n",
    "title = soup.select('#app > div > div > div.ArticleContentBox > div.article_header > div.ArticleTitle > div > h3')\n",
    "    # 내용을 하나의 텍스트로 만든다. (띄어쓰기 단위)\n",
    "content_tags = soup.select('#app > div > div > div.ArticleContentBox > div.article_container > div.article_viewer')\n",
    "content = ' '.join([ tags.get_text() for tags in content_tags ])\n",
    "# dict형태로 만들어 결과 list에 저장\n",
    "res_list.append({'title' : title, 'content' : content})\n",
    "# time.sleep 작업도 필요하다.\n",
    "# 결과 데이터프레임화\n",
    "cafe_df = pd.DataFrame(res_list)\n",
    "# csv파일로 추출\n",
    "cafe_df.to_csv('cafe_crawling.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee20fd5-e044-4690-a575-c91a133edb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d369741-9667-4fc3-8d67-75cfe9abc95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adc681e-2ca4-494e-844a-993fc171a04a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcec894-89ca-4248-87ed-c135a329aa10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec3558-b5d1-455c-8399-afeaf84aff41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e93bf80-2f4a-4581-9da6-f8155f90dce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d265e7-84be-4688-b21e-f8cb7cecfed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e649ed6b-7dee-4cdb-a12f-d182ed5f455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# href 속성을 찾아 url을 리스트로 저장한다.\n",
    "    ## 각 카페마다 태그의 형식이 다를 수 있으니 확인해야 한다. ( 태그.클래스명 )\n",
    "    article_list = driver.find_elements_by_css_selector('span.aaa > a.m-tcol-c')\n",
    "    article_urls = [ i.get_attribute('href') for i in article_list ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bffd775-a8cf-4dab-b793-3e353e0de836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae7fe257-a07c-4276-8d03-30571e146702",
   "metadata": {},
   "source": [
    "## 카페 게시글 내용 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae0cd1-3f33-403e-aae0-878162a70c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from contextlib import suppress\n",
    "\n",
    "for keyword in cafe.get(\"keywords\"):\n",
    "    df = pickle.load(open(save_path+\"cafe_address_\"+cafe.get(\"name\")+\"_\"+keyword+\".pkl\", 'rb'))\n",
    "\n",
    "    i=0\n",
    "    contents_list = []   # 내용\n",
    "    reply_list = []      # 댓글\n",
    "    error_list = []      # 에러난 게시글\n",
    "\n",
    "    while True:\n",
    "\n",
    "        ### 수집 링크로 이동\n",
    "        url = \"https://cafe.naver.com\"+df.loc[i,'link']\n",
    "        idx_no = df.loc[i,'idx_no']    # 인덱스 번호\n",
    "        driver.get(url)\n",
    "        time.sleep( random.randint(2,5) )\n",
    "        try:\n",
    "            driver.switch_to.frame('cafe_main')\n",
    "            time.sleep( random.randint(2,5) )\n",
    "            r = driver.page_source\n",
    "            page_soup = BeautifulSoup(r, \"html.parser\")\n",
    "            content = page_soup.find('div', class_='ArticleContentBox')  \n",
    "\n",
    "            ### 게시글 수집\n",
    "            temp_dict={}\n",
    "            temp_dict['idx_no'] = idx_no\n",
    "            temp_dict['title'] = \"\"\n",
    "            with suppress(AttributeError):   # 제목 없는 게시글\n",
    "                temp_dict['title'] = content.find('h3',class_='title_text').text.strip()\n",
    "            temp_dict['content'] = content.find(\"div\", class_=\"article_viewer\").text.strip()\n",
    "            temp_dict['nick'] = content.find('div',class_='profile_info').find('a',class_='nickname').text.strip()\n",
    "            temp_dict['date'] = content.find('div',class_='article_info').find('span',class_='date').text.strip()\n",
    "            temp_dict['view'] = \"\"\n",
    "            with suppress(AttributeError):\n",
    "                temp_dict['view'] = content.find('div',class_='article_info').find('span',class_='count').text.strip()\n",
    "            contents_list.append(temp_dict)\n",
    "\n",
    "            ### 댓글 수집\n",
    "            if content.find(\"div\", class_=\"ReplyBox\") is not None:   # 댓글 기능이 아예 없음  \n",
    "                comment_num = content.find(\"div\", class_=\"ReplyBox\").find(\"a\",class_=\"button_comment\").find(\"strong\").text\n",
    "                if comment_num!='0':   # 댓글이 없음\n",
    "                    comment = content.find(\"div\", class_=\"CommentBox\").find(\"ul\",class_=\"comment_list\").select(\"li\")\n",
    "                    \n",
    "                    ### 댓글 구분\n",
    "                    com_n=0    # 댓글\n",
    "                    com_nn=0   # 대댓글\n",
    "                    \n",
    "                    for n in range(len(comment)):\n",
    "\n",
    "                        if comment[n].get('class')==['CommentItem']:    # 댓글\n",
    "                            com_n+=1; com_nn=0;\n",
    "                            com_thread = str(com_n)+\"-\"+str(com_nn)\n",
    "                            com_nn=1\n",
    "                        elif comment[n].get('class')==['CommentItem', 'CommentItem--reply']:    # 대댓글\n",
    "                            com_thread = str(com_n)+\"-\"+str(com_nn)\n",
    "                            com_nn+=1\n",
    "\n",
    "                        ### 댓글 내용 수집    \n",
    "                        if comment[n].text.strip() != '삭제된 댓글입니다.':\n",
    "                            com_nick = comment[n].find(\"a\",class_=\"comment_nickname\").text.strip()\n",
    "                            com_date = comment[n].find(\"span\",class_=\"comment_info_date\").text.strip()\n",
    "                            com_reply = comment[n].find(\"div\",class_=\"comment_text_box\").text.strip()\n",
    "                            reply_list.append({'idx_no':idx_no, 'nick':com_nick, 'date':com_date, 'reply':com_reply, \"thread\":com_thread})\n",
    "            i+=1\n",
    "\n",
    "        except:\n",
    "            i+=1\n",
    "            ### 게시글을 볼 등급이 안됨\n",
    "            if page_soup.find('strong', class_='emph') is not None:\n",
    "                error_list.append({\"error\" :  page_soup.find('strong', class_='emph').text+\"등급 필요\"\n",
    "                                   , \"url\" : url})\n",
    "                pass\n",
    "            ### 에러 따로 확인\n",
    "            else:\n",
    "                error_list.append({\"error\" : \"에러 확인 필요\"\n",
    "                                   , \"url\" : url})\n",
    "                pass\n",
    "\n",
    "        ### 수집한 글 갯수만큼 반복\n",
    "        if i == len(df):\n",
    "            contents_df = pd.DataFrame(contents_list)\n",
    "            contents_df.to_pickle(\"../../data/cafe/cafe_contents_\"+cafe.get(\"name\")+\"_\"+keyword+\".pkl\")\n",
    "            reply_df = pd.DataFrame(reply_list)\n",
    "            reply_df.to_pickle(\"../../data/cafe/cafe_replies_\"+cafe.get(\"name\")+\"_\"+keyword+\".pkl\")\n",
    "            print(\"(현재시각) \"+str(datetime.datetime.now())+\": done\")\n",
    "            break\n",
    "    \n",
    "# 크롬 종료 \n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
